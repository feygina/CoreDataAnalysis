---
title: "Untitled"
output: pdf_document

---

# **Problem 7.1**

```{r, include=FALSE}
data = read.csv("data.csv", sep = ",")
library(vegan)
library(stats)
require(graphics)
```

**Function helping to make right chouse about number of K**

```{r}
wssplot <- function(data, nc=15, seed=1234){
               wss <- (nrow(data)-1)*sum(apply(data,2,var))
               for (i in 2:nc){
                    set.seed(seed)
                    wss[i] <- sum(kmeans(data, centers=i)$withinss)}
                plot(1:nc, wss, type="b", xlab="Number of Clusters",
                     ylab="Within groups sum of squares")}
```

**Data that we want to use for clusterization**

```{r}
kmeans.data <- data[4:6]
head(kmeans.data)
```

**Scaling the data**

```{r}
kmeans.data.scaled <- scale(kmeans.data[-1])
wssplot(kmeans.data.scaled)
```

**Let's do some clusterization**

**3 clusters**

```{r}
# K-Means Cluster Analysis
fit3 <- kmeans(kmeans.data.scaled, 3)
# get cluster means 
aggregate(kmeans.data.scaled,by=list(fit3$cluster),FUN=mean)
# append cluster assignment
kmeans.data.scaled <- data.frame(kmeans.data.scaled, fit3$cluster)
```

**Cluster Plot against 1st 2 principal components**

```{r}
# vary parameters for most readable graph
library(cluster) 
clusplot(kmeans.data.scaled, fit3$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)

#Centroid Plot against 1st 2 discriminant functions

#library(fpc)
#plotcluster(kmeans.data, fit$cluster)
```

**4 clusters**

```{r}
# K-Means Cluster Analysis
fit4 <- kmeans(kmeans.data.scaled, 4)
# get cluster means 
aggregate(kmeans.data.scaled,by=list(fit4$cluster),FUN=mean)
# append cluster assignment
kmeans.data.scaled <- data.frame(kmeans.data.scaled, fit4$cluster)
```

**Cluster Plot against 1st 2 principal components**

```{r}
# vary parameters for most readable graph
clusplot(kmeans.data.scaled, fit4$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
```

**7 clusters**

```{r}
# K-Means Cluster Analysis
fit7 <- kmeans(kmeans.data.scaled, 7)
# get cluster means 
aggregate(kmeans.data.scaled,by=list(fit7$cluster),FUN=mean)
# append cluster assignment
kmeans.data.scaled <- data.frame(kmeans.data.scaled, fit7$cluster)
```

**Cluster Plot against 1st 2 principal components**

```{r}
# vary parameters for most readable graph
clusplot(kmeans.data.scaled, fit7$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
```

**comparing 2 cluster solutions 3-means and 4-means**

```{r}
library(fpc)
d <- dist(kmeans.data.scaled)
cluster.stats(d, fit3$cluster, fit4$cluster)
```

**The correct choice of k is often ambiguous, with interpretations depending on the shape and scale of the distribution of points in a data set and the desired clustering resolution of the user. In addition, increasing k without penalty will always reduce the amount of error in the resulting clustering, to the extreme case of zero error if each data point is considered its own cluster (i.e., when k equals the number of data points, n). Intuitively then, the optimal choice of k will strike a balance between maximum compression of the data using a single cluster, and maximum accuracy by assigning each data point to its own cluster.**

**So, in our case we see that the total within-cluster sum of squares is redusing as k is getting bigger.**

\newpage

# **Problem 7.2**

**Build a minimum spanning tree connecting all points and delete 6 maximum edges step by step.**

```{r, include=TRUE}
stdData <- kmeans.data.scaled
dis <- dist(stdData) #dissimilarity data
tr <- spantree(dis, toolong = 0) #build an MST
plot(tr, cmdscale(dis)) #plot an MST
for(i in 1:6) #loop for finding 6 maximum edges in MST
{
  m <- which.max(tr$dist)
  tr$dist[m] <- NA
}
m <- which.max(tr$dist)
tres <- tr$dist[m] + 0.00001 #set a treshold
j <- distconnected(dis, toolong = tres) #find connected components in MST after deleting 6 edges
clusplot(stdData, j, color=TRUE, shade=TRUE) #plot clusters
```


```{r, include=TRUE}
hc <- hclust(dist(stdData), method = "single")
plot(hc, labels = FALSE)
memb <- cutree(hc, k = 7)
clusplot(stdData, memb, color=TRUE, shade=TRUE)
```



**Consider a set of 2D points presented. Those in problem 2 clustered by using the single link approach, whereas those in problem 1, by using the square error criterion of K-Means.**


**Overall, this example demonstrates the major difference between conventional clustering and single link clustering: the latter finds elongated structures whereas the former cuts out convex parts. Sometimes, especially in the analysis of results of physical processes or experiments over real-world particles, the elongated structures do capture the essence of the data and are of great interest. In other cases, especially when entities/features have no intuitive geometric meaning ??? think of hockey players, for example, convex clusters make much more sense as groupings around their centroids.**